---
---

@string{aps = {American Physical Society,}}

@book{hsieh2025dplm2-1,
  type={article},
  abstract={Multimodal protein language models (PLMs) integrate sequence and token-based structural information,
   serving as a powerful foundation for protein modeling, generation, and design. 
   However, the reliance on tokenizing 3D structures into discrete tokens causes substantial 
   loss of fidelity about fine-grained structural details and correlations. 
   In this paper, we systematically elucidate the design space of multimodal PLMs to overcome their limitations. 
   We identify tokenization loss and inaccurate structure token predictions by the PLMs as major bottlenecks. 
   To address these, our proposed design space covers improved generative modeling, 
   structure-aware architectures and representation learning, and data exploration. 
   Our advancements approach finer-grained supervision, demonstrating that token-based multimodal PLMs 
   can achieve robust structural modeling. The effective design methods dramatically improve the structure generation diversity, 
   and notably, folding abilities of our 650M model by reducing the RMSD from 5.52 to 2.36 on PDB testset, 
   even outperforming 3B baselines and on par with the specialized folding models.},
  title={Elucidating the Design Space of Multimodal Protein Language Models},
  title_html={https://bytedance.github.io/dplm/dplm-2.1/},
  teaser_html={https://bytedance.github.io/dplm/dplm-2.1/},
  author={Cheng-Yen Hsieh and Xinyou Wang and Daiheng Zhang and Dongyu Xue and Fei Ye and Shujian Huang and Zaixiang Zheng and Quanquan Gu},
  publisher={arXiv},
  journal={in Submission},
  booktitle={arXiv},
  conference={arXiv},
  volume={},
  issue={},
  pages={},
  numpages={},
  year={2025},
  month={},
  code={https://github.com/bytedance/dplm},
  pdf={https://arxiv.org/abs/2504.11454},
  selected={true},
  preview={/assets/video/DPLM-2.1.mp4},
  preview_img={/assets/img/publication_preview/dplm-2.1.png},
  note={Design choices are essential: Our designs enable the 650M multimodal PLM to outperform 3B-scale baselines and specialized structure folding models.},
}

@book{hsieh2023taoamodal,
  type={article},
  abstract={Amodal perception, the ability to comprehend complete object structures from partial visibility, 
    is a fundamental skill, even for infants. Its significance extends to applications like autonomous driving,
    where a clear understanding of heavily occluded objects is essential. However, modern detection and tracking 
    algorithms often overlook this critical capability, perhaps due to the prevalence of modal annotations in most 
    benchmarks. To address the scarcity of amodal benchmarks, we introduce TAO-Amodal, featuring 833 diverse 
    categories in thousands of video sequences. Our dataset includes amodal and modal bounding boxes for visible 
    and partially or fully occluded objects, including those that are partially out of the camera frame. 
    We investigate the current lay of the land in both amodal tracking and detection by benchmarking state-of-the-art 
    modal trackers and amodal segmentation methods. We find that existing methods, even when adapted for amodal 
    tracking, struggle to detect and track objects under heavy occlusion. To mitigate this, we explore simple finetuning 
    schemes that can increase the amodal tracking and detection metrics of occluded objects by 2.1% and 3.3%.},
  title={TAO-Amodal: A Benchmark for Tracking Any Object Amodally},
  title_html={https://tao-amodal.github.io/},
  html={https://tao-amodal.github.io/},
  teaser_html={https://tao-amodal.github.io/},
  author={Hsieh, Cheng-Yen and Kaihua Chen and Dave, Achal and Tarasha Khurana and Deva Ramanan},
  publisher={arXiv},
  journal={in Submission},
  booktitle={arXiv},
  conference={arXiv},
  volume={},
  issue={},
  pages={},
  numpages={},
  year={2023},
  month={Nov.},
  code={https://github.com/WesleyHsieh0806/TAO-Amodal},
  pdf={https://arxiv.org/abs/2312.12433},
  selected={true},
  preview={/assets/video/tao_amodal.mp4},
  preview_img={/assets/img/publication_preview/tao_amodal.png},
  note={Our solution to unravel occlusion scenarios for any objectâ€”amodal tracking.},
}

@book{hsieh2023self,
  type={article},
  abstract={While self-supervised learning has been shown to benefit a number of vision tasks, existing techniques mainly focus on image-level manipulation, which may not generalize well to downstream tasks at patch or pixel levels. Moreover, 
    existing SSL methods might not sufficiently describe and associate the above representations within and across image scales. 
    In this paper, we propose a Self-Supervised Pyramid Representation Learning (SS-PRL) framework. The
    proposed SS-PRL is designed to derive pyramid representations at patch levels via learning proper prototypes, with
    additional learners to observe and relate inherent semantic information within an image. In particular, we present
    a cross-scale patch-level correlation learning in SS-PRL,
    which allows the model to aggregate and associate information learned across patch scales. We show that, with
    our proposed SS-PRL for model pre-training, one can easily adapt and fine-tune the models for a variety of applications including multi-label classification, object detection,
    and instance segmentation.},
  title={Self-Supervised Pyramid Representation Learning for Multi-Label Visual Analysis and Beyond},
  title_html={https://github.com/WesleyHsieh0806/SS-PRL},
  teaser_html={https://github.com/WesleyHsieh0806/SS-PRL},
  author={Hsieh, Cheng-Yen and Chang, Chih-Jung and Yang, Fu-En and Wang, Yu-Chiang Frank},
  publisher={arxiv},
  journal={IEEE WACV},
  booktitle={IEEE WACV},
  conference={WACV},
  volume={},
  issue={},
  pages={},
  numpages={},
  year={2023},
  month={},
  code={https://github.com/WesleyHsieh0806/SS-PRL},
  pdf={https://openaccess.thecvf.com/content/WACV2023/papers/Hsieh_Self-Supervised_Pyramid_Representation_Learning_for_Multi-Label_Visual_Analysis_and_Beyond_WACV_2023_paper.pdf},
  selected={true},
  preview={https://github.com/WesleyHsieh0806/SS-PRL/raw/master/GIF/Framework%20Gif.gif},
  note={One can easily adapt and fine-tune the models for a variety of applications including multi-label classification, object detection,and instance segmentation with this pre-training algorithm.},
}

@book{hsieh2022c3,
  type={article},
  abstract={Most existing studies improve the efficiency of Split learning
            (SL) by compressing the transmitted features. However, most
            works focus on dimension-wise compression that transforms
            high-dimensional features into a low-dimensional space. In
            this paper, we propose circular convolution-based batch-wise
            compression for SL (C3-SL) to compress multiple features
            into one single feature. To avoid information loss while merging multiple features, we exploit the quasi-orthogonality of
            features in high-dimensional space with circular convolution
            and superposition. To the best of our knowledge, we are
            the first to explore the potential of batch-wise compression
            under the SL scenario. Based on the simulation results on
            CIFAR-10 and CIFAR-100, our method achieves a 16x compression ratio with negligible accuracy drops compared with
            the vanilla SL. Moreover, C3-SL significantly reduces 1152x
            memory and 2.25x computation overhead compared to the
            state-of-the-art dimension-wise compression method.},
  title={C3-SL: Circular Convolution-Based Batch-Wise Compression for Communication-Efficient Split Learning},
  title_html={https://github.com/WesleyHsieh0806/C3-SL},
  teaser_html={https://github.com/WesleyHsieh0806/C3-SL},
  author={Cheng-Yen Hsieh and Yu-Chuan Chuang and An-Yeu (Andy) Wu},
  publisher={},
  journal={IEEE 32nd International Workshop on Machine Learning for Signal Processing (MLSP)},
  booktitle={},
  conference={},
  volume={},
  issue={},
  pages={},
  numpages={},
  year={2022},
  month={},
  code={https://github.com/WesleyHsieh0806/C3-SL},
  pdf={https://arxiv.org/pdf/2207.12397.pdf},
  selected={true},
  preview={c3-sl.png},
  note={Split Learning (SL) for efficient image recognition through dimension-wise compression.},
}


@book{hsieh2021fl,
  type={article},
  abstract={Federated learning (FL) is a privacy-preserving learning framework, which collaboratively learns a centralized model across edge devices. 
            Each device trains an independent model with its local dataset and only uploads model parameters to mitigate privacy concerns. However, 
            most FL works focus on deep neural networks (DNNs), whose intensive computation hinders FL from practical realization on resource-limited edge devices. 
            In this paper, we exploit the high energy efficiency properties of hyperdimensional computing (HDC) to propose a federated learning HDC (FL-HDC). In FL-HDC, 
            we bipolarize model parameters to significantly reduce communication costs, which is a primary concern in FL. Moreover, we propose a retraining mechanism with 
            adaptive learning rates to compensate for the accuracy degradation caused by bipolarization. Under the FL scenario, our simulation results show the effectiveness
             of our proposed FL-HDC across two datasets, MNIST and ISOLET. Compared with the previous work that transmits complete model parameters to the cloud, FL-HDC greatly
              reduces 23x and 9x communication costs with comparable accuracy in ISOLET and MNIST, respectively.},
  title={FL-HDC: Hyperdimensional Computing Design for the Application of Federated Learning},
  title_html={https://github.com/WesleyHsieh0806/FL-HDC},
  teaser_html={https://github.com/WesleyHsieh0806/FL-HDC},
  author={Cheng-Yen Hsieh and Yu-Chuan Chuang and An-Yeu (Andy) Wu},
  publisher={},
  journal={IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS)},
  booktitle={},
  conference={},
  volume={},
  issue={},
  pages={},
  numpages={},
  year={2021},
  month={},
  code={https://github.com/WesleyHsieh0806/FL-HDC},
  pdf={https://ieeexplore.ieee.org/document/9458526},
  selected={true},
  preview={fl-hdc.png},
  note={Highly efficienct image recognition under the federated learning (FL) scenario.},
}