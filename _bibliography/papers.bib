---
---

@string{aps = {American Physical Society,}}

@book{hsieh2023self,
  type={article},
  abstract={Amodal perception, the ability to comprehend complete object structures from partial visibility, is a fundamental skill, even for infants. 
  Its significance extends to applications like autonomous driving, where a clear understanding of heavily occluded objects is essential. 
  However, modern detection and tracking algorithms often overlook this critical capability, perhaps due to the prevalence of modal annotations in most datasets. 
  To address the scarcity of amodal data, we introduce the TAO-Amodal benchmark, featuring 880 diverse categories in thousands of video sequences. 
  Our dataset includes amodal and modal bounding boxes for visible and occluded objects, including objects that are partially out-of-frame. 
  To enhance amodal tracking with object permanence, we leverage a lightweight plug-in module, the amodal expander, to transform standard, modal 
  trackers into amodal ones through fine-tuning on a few hundred video sequences with data augmentation. We achieve a 3.3% and 1.6% improvement on 
  the detection and tracking of occluded objects on TAO-Amodal. When evaluated on people, our method produces dramatic improvements of 2x compared 
  to state-of-the-art modal baselines.},
  title={Tracking Any Object Amodally},
  title_html={https://tao-amodal.github.io/},
  html={https://tao-amodal.github.io/},
  author={Hsieh, Cheng-Yen and Tarasha Khurana and Dave, Achal and Deva Ramanan},
  publisher={arXiv},
  journal={in Submission},
  booktitle={arXiv},
  conference={arXiv},
  volume={},
  issue={},
  pages={},
  numpages={},
  year={2023},
  month={},
  selected={true},
  preview={/assets/video/tao_amodal.mp4},
  preview_img={/assets/img/publication_preview/tao_amodal.png},
  note={Our solution to unravel occlusion scenarios for any object, amodal tracking.},
}

@book{hsieh2023self,
  type={article},
  abstract={While self-supervised learning has been shown to benefit a number of vision tasks, existing techniques mainly focus on image-level manipulation, which may not generalize well to downstream tasks at patch or pixel levels. Moreover, 
    existing SSL methods might not sufficiently describe and associate the above representations within and across image scales. 
    In this paper, we propose a Self-Supervised Pyramid Representation Learning (SS-PRL) framework. The
    proposed SS-PRL is designed to derive pyramid representations at patch levels via learning proper prototypes, with
    additional learners to observe and relate inherent semantic information within an image. In particular, we present
    a cross-scale patch-level correlation learning in SS-PRL,
    which allows the model to aggregate and associate information learned across patch scales. We show that, with
    our proposed SS-PRL for model pre-training, one can easily adapt and fine-tune the models for a variety of applications including multi-label classification, object detection,
    and instance segmentation.},
  title={Self-Supervised Pyramid Representation Learning for Multi-Label Visual Analysis and Beyond},
  title_html={https://github.com/WesleyHsieh0806/SS-PRL},
  html={https://github.com/WesleyHsieh0806/SS-PRL},
  author={Hsieh, Cheng-Yen and Chang, Chih-Jung and Yang, Fu-En and Wang, Yu-Chiang Frank},
  publisher={arxiv},
  journal={IEEE WACV},
  booktitle={IEEE WACV},
  conference={WACV},
  volume={},
  issue={},
  pages={},
  numpages={},
  year={2023},
  month={},
  code={https://github.com/WesleyHsieh0806/SS-PRL},
  pdf={https://openaccess.thecvf.com/content/WACV2023/papers/Hsieh_Self-Supervised_Pyramid_Representation_Learning_for_Multi-Label_Visual_Analysis_and_Beyond_WACV_2023_paper.pdf},
  selected={true},
  preview={https://github.com/WesleyHsieh0806/SS-PRL/raw/master/GIF/Framework%20Gif.gif},
  note={One can easily adapt and fine-tune the models for a variety of applications including multi-label classification, object detection,and instance segmentation with this pre-training algorithm.},
}
