<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Cheng-Yen (Wesley) Hsieh</title> <meta name="author" content="Cheng-Yen (Wesley) Hsieh"> <meta name="description" content="Cheng-Yen (Wesley) Hsieh's research portfolio. "> <meta name="keywords" content="cheng-yen hsieh, wesley hsieh, cheng-yen (wesley) hsieh, computer vision, cmu, mscv, carnegie mellon university, object detection, object tracking, self-supervised learning, vision-language models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?2edcb72ad58363ff7c2900d3eeb8fc87" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/Carnegie_Mellon_University_seal.png?52a6539acfe6aaa9d7710fa049d44725"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wesleyhsieh0806.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/Cheng-Yen-Hsieh_Resume_Research.pdf">resume</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <a href="" role="button"><span class="font-weight-bold">Cheng-Yen</span> (Wesley) Hsieh</a> </h1> <p class="desc">CMU RI student | Machine Learning Research Scientist</p> </header> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Wesley-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Wesley-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Wesley-1400.webp"></source> <p><a href=""> <img src="/assets/img/Wesley.jpg?d326fdc75cd21cef2d8a4761b898c206" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="Wesley.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </a></p> </picture> </figure> </div> <div class="clearfix"> <p>I am an incoming research scientist at Tiktok studying multi-modal large language agents. I obtained my master degree in computer vision at <a href="https://www.cmu.edu/" rel="external nofollow noopener" target="_blank">Carnegie Mellon University</a>, where I was advised by Prof. <a href="https://www.cs.cmu.edu/~deva/" rel="external nofollow noopener" target="_blank">Deva Ramanan</a>. My research experience lies in the fields of machine learning (ML) and computer vision (CV), including topics like <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Hsieh_Self-Supervised_Pyramid_Representation_Learning_for_Multi-Label_Visual_Analysis_and_Beyond_WACV_2023_paper.pdf" rel="external nofollow noopener" target="_blank">self-supervised learning</a>, <a href="https://tao-amodal.github.io" rel="external nofollow noopener" target="_blank">object tracking</a>, and vision-language models. More specifically, my research pursuits are centered around the development of algorithms that enhance perceptual capabilities under challenging conditions, such as occlusion, leveraging minimal supervision and multimodal information.</p> <p>Prior to my master journey, I received my B.S. from <a href="https://www.ntu.edu.tw/english/" rel="external nofollow noopener" target="_blank">National Taiwan University</a>. I had the pleasure to work on self-supervised representation learning with Prof. <a href="http://vllab.ee.ntu.edu.tw/members.html" rel="external nofollow noopener" target="_blank">Yu-Chiang Frank Wang</a> and <a href="http://access.ee.ntu.edu.tw/Publications/Conference/(2021)%20FL-HDC_Hyperdimensional_Computing_Design_for_the_Application_of_Federated_Learning.pdf" rel="external nofollow noopener" target="_blank">federated learning</a> with Prof. <a href="http://access.ee.ntu.edu.tw/" rel="external nofollow noopener" target="_blank">An-Yeu (Andy) Wu</a>.</p> <p><br> <br> <br> <br></p> <p><a href="https://github.com/WesleyHsieh0806" rel="external nofollow noopener" target="_blank">Github</a> / <a href="https://scholar.google.com/citations?user=xUFnq1oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Google Scholar</a> / <a href="mailto:chengyenhsieh0806@gmail.com">chengyenhsieh0806@gmail.com</a></p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Dec 5, 2023</th> <td> Actively seeking for 2024 Fall PhD positions. </td> </tr> <tr> <th scope="row">Aug 22, 2022</th> <td> I joined CMU RI as a master student in computer vision. </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-7 preview"><p><a href="https://tao-amodal.github.io/" rel="external nofollow noopener" target="_blank"> <video class="preview z-depth-1 rounded" muted="" playsinline="" autoplay="" loop="" poster="/assets/img/publication_preview/tao_amodal.png" preload="auto"> <source src="/assets/video/tao_amodal.mp4" type="video/mp4"> </source></video> </a></p></div> <div id="hsieh2023taoamodal" class="col-sm-8"> <div class="title"> <a href="https://tao-amodal.github.io/" role="button" rel="external nofollow noopener" target="_blank">Tracking Any Object Amodally</a> </div> <div class="author"> <em>Cheng-Yen Hsieh</em>, <a href="https://www.cs.cmu.edu/~tkhurana/" rel="external nofollow noopener" target="_blank">Tarasha Khurana</a>, <a href="https://www.achaldave.com/" rel="external nofollow noopener" target="_blank">Achal Dave</a>, and <a href="https://www.cs.cmu.edu/~deva/" rel="external nofollow noopener" target="_blank">Deva Ramanan</a> </div> <div class="periodical"> <em>in Submission</em>, Nov 2023 </div> <div class="periodical"> Our solution to unravel occlusion scenarios for any object—amodal tracking. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://tao-amodal.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Page</a> <a href="https://arxiv.org/abs/2312.12433" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Amodal perception, the ability to comprehend complete object structures from partial visibility, is a fundamental skill, even for infants. Its significance extends to applications like autonomous driving, where a clear understanding of heavily occluded objects is essential. However, modern detection and tracking algorithms often overlook this critical capability, perhaps due to the prevalence of modal annotations in most datasets. To address the scarcity of amodal data, we introduce the TAO-Amodal benchmark, featuring 880 diverse categories in thousands of video sequences. Our dataset includes amodal and modal bounding boxes for visible and occluded objects, including objects that are partially out-of-frame. To enhance amodal tracking with object permanence, we leverage a lightweight plug-in module, the amodal expander, to transform standard, modal trackers into amodal ones through fine-tuning on a few hundred video sequences with data augmentation. We achieve a 3.3% and 1.6% improvement on the detection and tracking of occluded objects on TAO-Amodal. When evaluated on people, our method produces dramatic improvements of 2x compared to state-of-the-art modal baselines.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-7 preview"><p><a href="https://github.com/WesleyHsieh0806/SS-PRL" rel="external nofollow noopener" target="_blank"> <img class="preview z-depth-1 rounded" src="https://github.com/WesleyHsieh0806/SS-PRL/raw/master/GIF/Framework%20Gif.gif"> </a></p></div> <div id="hsieh2023self" class="col-sm-8"> <div class="title"> <a href="https://github.com/WesleyHsieh0806/SS-PRL" role="button" rel="external nofollow noopener" target="_blank">Self-Supervised Pyramid Representation Learning for Multi-Label Visual Analysis and Beyond</a> </div> <div class="author"> <em>Cheng-Yen Hsieh</em>, Chih-Jung Chang, Fu-En Yang, and <a href="http://vllab.ee.ntu.edu.tw/members.html" rel="external nofollow noopener" target="_blank">Yu-Chiang Frank Wang</a> </div> <div class="periodical"> <em>IEEE WACV</em>, 2023 </div> <div class="periodical"> One can easily adapt and fine-tune the models for a variety of applications including multi-label classification, object detection,and instance segmentation with this pre-training algorithm. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Hsieh_Self-Supervised_Pyramid_Representation_Learning_for_Multi-Label_Visual_Analysis_and_Beyond_WACV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/WesleyHsieh0806/SS-PRL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>While self-supervised learning has been shown to benefit a number of vision tasks, existing techniques mainly focus on image-level manipulation, which may not generalize well to downstream tasks at patch or pixel levels. Moreover, existing SSL methods might not sufficiently describe and associate the above representations within and across image scales. In this paper, we propose a Self-Supervised Pyramid Representation Learning (SS-PRL) framework. The proposed SS-PRL is designed to derive pyramid representations at patch levels via learning proper prototypes, with additional learners to observe and relate inherent semantic information within an image. In particular, we present a cross-scale patch-level correlation learning in SS-PRL, which allows the model to aggregate and associate information learned across patch scales. We show that, with our proposed SS-PRL for model pre-training, one can easily adapt and fine-tune the models for a variety of applications including multi-label classification, object detection, and instance segmentation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-7 preview"><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/c3-sl-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/c3-sl-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/c3-sl-1400.webp"></source> <p><a href="https://github.com/WesleyHsieh0806/C3-SL" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/publication_preview/c3-sl.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="c3-sl.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </a></p> </picture> </figure></div> <div id="hsieh2022c3" class="col-sm-8"> <div class="title"> <a href="https://github.com/WesleyHsieh0806/C3-SL" role="button" rel="external nofollow noopener" target="_blank">C3-SL: Circular Convolution-Based Batch-Wise Compression for Communication-Efficient Split Learning</a> </div> <div class="author"> <em>Cheng-Yen Hsieh</em>, Yu-Chuan Chuang, and <a href="http://access.ee.ntu.edu.tw/" rel="external nofollow noopener" target="_blank">An-Yeu (Andy) Wu</a> </div> <div class="periodical"> <em>IEEE 32nd International Workshop on Machine Learning for Signal Processing (MLSP)</em>, 2022 </div> <div class="periodical"> Split Learning (SL) for efficient image recognition through dimension-wise compression. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2207.12397.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/WesleyHsieh0806/C3-SL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Most existing studies improve the efficiency of Split learning (SL) by compressing the transmitted features. However, most works focus on dimension-wise compression that transforms high-dimensional features into a low-dimensional space. In this paper, we propose circular convolution-based batch-wise compression for SL (C3-SL) to compress multiple features into one single feature. To avoid information loss while merging multiple features, we exploit the quasi-orthogonality of features in high-dimensional space with circular convolution and superposition. To the best of our knowledge, we are the first to explore the potential of batch-wise compression under the SL scenario. Based on the simulation results on CIFAR-10 and CIFAR-100, our method achieves a 16x compression ratio with negligible accuracy drops compared with the vanilla SL. Moreover, C3-SL significantly reduces 1152x memory and 2.25x computation overhead compared to the state-of-the-art dimension-wise compression method.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-7 preview"><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/fl-hdc-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/fl-hdc-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/fl-hdc-1400.webp"></source> <p><a href="https://github.com/WesleyHsieh0806/FL-HDC" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/publication_preview/fl-hdc.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="fl-hdc.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </a></p> </picture> </figure></div> <div id="hsieh2021fl" class="col-sm-8"> <div class="title"> <a href="https://github.com/WesleyHsieh0806/FL-HDC" role="button" rel="external nofollow noopener" target="_blank">FL-HDC: Hyperdimensional Computing Design for the Application of Federated Learning</a> </div> <div class="author"> <em>Cheng-Yen Hsieh</em>, Yu-Chuan Chuang, and <a href="http://access.ee.ntu.edu.tw/" rel="external nofollow noopener" target="_blank">An-Yeu (Andy) Wu</a> </div> <div class="periodical"> <em>IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS)</em>, 2021 </div> <div class="periodical"> Highly efficienct image recognition under the federated learning (FL) scenario. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/9458526" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/WesleyHsieh0806/FL-HDC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Federated learning (FL) is a privacy-preserving learning framework, which collaboratively learns a centralized model across edge devices. Each device trains an independent model with its local dataset and only uploads model parameters to mitigate privacy concerns. However, most FL works focus on deep neural networks (DNNs), whose intensive computation hinders FL from practical realization on resource-limited edge devices. In this paper, we exploit the high energy efficiency properties of hyperdimensional computing (HDC) to propose a federated learning HDC (FL-HDC). In FL-HDC, we bipolarize model parameters to significantly reduce communication costs, which is a primary concern in FL. Moreover, we propose a retraining mechanism with adaptive learning rates to compensate for the accuracy degradation caused by bipolarization. Under the FL scenario, our simulation results show the effectiveness of our proposed FL-HDC across two datasets, MNIST and ISOLET. Compared with the previous work that transmits complete model parameters to the cloud, FL-HDC greatly reduces 23x and 9x communication costs with comparable accuracy in ISOLET and MNIST, respectively.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%63%68%65%6E%67%79%65%6E%68%73%69%65%68%30%38%30%36@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=xUFnq1oAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/WesleyHsieh0806" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/chengyen-hsieh" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/chengyenhsieh" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> <a href="https://medium.com/@chengyenhsieh0806" title="Medium" rel="external nofollow noopener" target="_blank"><i class="fab fa-medium"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> <div class="contact-note"> I am actively looking for PhD positions in ML or CV </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Cheng-Yen (Wesley) Hsieh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>